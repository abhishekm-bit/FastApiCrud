#ğŸ“„ PDF Similarity Search using RAG (Ollama + ChromaDB)
---
---
This project demonstrates a basic Retrieval-Augmented Generation (RAG) pipeline built from scratch using Python, Ollama embeddings, ChromaDB (Vector Database), and Cosine Similarity.
---
---
##The system:
Loads a PDF

Converts PDF text into chunks

Generates embeddings for each chunk

Stores embeddings in a vector database (ChromaDB)

Accepts user queries

Finds the most semantically similar chunks using cosine similarity

ğŸ›  Tech Stack

Python 3.11

Virtual Environment (venv)

Ollama (nomic-embed-text model)

ChromaDB (vector database)

PyPDF

NumPy

Cosine Similarity

ğŸ“ Project Folder Structure
```
8-01-2026/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.pdf                 # Input PDF file
â”‚
â”œâ”€â”€ chroma_db/                     # Persistent ChromaDB storage
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ pdf_loader.py              # Load PDF text
â”‚   â”œâ”€â”€ text_splitter.py           # Chunking logic
â”‚   â”œâ”€â”€ embedding.py               # Generate embeddings (Ollama)
â”‚   â”œâ”€â”€ vector_store.py            # ChromaDB connection
â”‚   â”œâ”€â”€ search.py                  # Store & query chunks
â”‚   â””â”€â”€ cosine_similarity.py       # Manual cosine similarity
â”‚
â”œâ”€â”€ ingest_pdf.py                  # PDF â†’ chunks â†’ embeddings â†’ DB
â”œâ”€â”€ query.py                       # Query â†’ similarity search
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ .gitignore                     # Ignored files
â”œâ”€â”€ README.md                      # Documentation
â””â”€â”€ venv/                          # Virtual environment

```
âš™ï¸ Setup Instructions (VERY IMPORTANT)
1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

2ï¸âƒ£ Activate Virtual Environment
Windows

```bash
venv\Scripts\activate
```
3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
Example requirements.txt:
```
chromadb
ollama
pypdf
numpy
```
4ï¸âƒ£ Install Ollama & Embedding Model
Install Ollama from:

```bash
https://ollama.com
```

Then pull embedding model:
```bash
ollama pull nomic-embed-text
```

ğŸ“¥ Step 1: Ingest PDF into Vector Database
Run:
```bash
python ingest_pdf.py
```
What happens internally:

PDF text is loaded

Text is split into chunks (500 chars with 50 overlap)

Each chunk is converted into an embedding

Embeddings are stored in ChromaDB

Example output:

```
PDF TEXT LENGTH: 1379
AFTER INGEST â†’ TOTAL CHUNKS IN DB: 4
âœ… PDF ingested and stored in vector DB

```

ğŸ” Step 2: Query the PDF

Run:

```bash
python query.py
```

Example query:
```bash
query = "Who is Vishwas Narayan Nangare Patil?"
```

Example output:
```
--- Result 1 ---
Score: 0.615
Vishwas Narayan Nangare Patil, IPS, PPMG is an Indian Police Service officer...

--- Result 2 ---
Score: 0.525
Positions held...
```

âœ” Higher score = more similar text

ğŸ§© How Chunking Works

```bash
def chunk_text(text, chunk_size=500, overlap=50):
```
Each chunk contains 500 characters

50 characters overlap ensures context is preserved

Total chunks depend on PDF text length

Example:

PDF length = 1379

Chunk size = 500

Result = 4 chunks


```
ğŸ§  What ChromaDB Stores

Each entry in ChromaDB contains:

id â†’ chunk_0, chunk_1, etc.

embedding â†’ list of ~768 numbers

document â†’ original text chunk
```

```
ğŸ§ª Why Virtual Environment is Required

Prevents version conflicts (NumPy, ChromaDB)

Keeps project isolated

Industry best practice

Avoids Python 3.14 instability issues
```

```
ğŸš€ What This Project Demonstrates

âœ” RAG fundamentals
âœ” Vector databases
âœ” Embeddings
âœ” Semantic search
âœ” Cosine similarity
âœ” Real-world GenAI workflow
```
